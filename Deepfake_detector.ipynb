{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6807e91-5c20-4fed-81e8-29df65334b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Imports and setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfb43cd-e022-4145-91e5-e0157f390c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… No .ipynb_checkpoints found.\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”¥ Step 0 â€” Clean .ipynb_checkpoints automatically from all dataset folders\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def clean_checkpoints(base_folder):\n",
    "    removed = 0\n",
    "    for root, dirs, files in os.walk(base_folder):\n",
    "        for d in dirs:\n",
    "            if d == '.ipynb_checkpoints':\n",
    "                path = os.path.join(root, d)\n",
    "                shutil.rmtree(path, ignore_errors=True)\n",
    "                print(\"ðŸ§¹ Removed:\", path)\n",
    "                removed += 1\n",
    "    if removed == 0:\n",
    "        print(\"âœ… No .ipynb_checkpoints found.\")\n",
    "    else:\n",
    "        print(f\"âœ… Cleaned {removed} folders successfully!\")\n",
    "\n",
    "clean_checkpoints(\"dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ebe735-bf57-4d82-b146-31075f8e2edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Fake', 'Real']\n",
      "âœ… Training with 3000 images (subset)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load dataset and take small subset for faster training\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH = 8  # small batch for CPU\n",
    "\n",
    "# Image transforms (resize + normalize)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load full datasets\n",
    "train_data_full = datasets.ImageFolder(\"dataset/train\", transform=train_transform)\n",
    "val_data = datasets.ImageFolder(\"dataset/val\", transform=test_transform)\n",
    "test_data = datasets.ImageFolder(\"dataset/test\", transform=test_transform)\n",
    "\n",
    "# âœ… Take only a small random subset for training\n",
    "subset_size = 3000\n",
    "indices = np.random.choice(len(train_data_full), subset_size, replace=False)\n",
    "train_data = Subset(train_data_full, indices)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH, shuffle=False)\n",
    "\n",
    "print(\"Classes:\", train_data_full.classes)\n",
    "print(f\"âœ… Training with {subset_size} images (subset)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "519ee3d4-ff7b-4d6f-939e-52eb24358749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ResNet18 model ready for training\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load Pretrained ResNet18 for Transfer Learning\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "# Load pre-trained ResNet18\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the final fully-connected layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace final layer for 2-class classification (Fake vs Real)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # 2 outputs\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "print(\"âœ… ResNet18 model ready for training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e3fd455-6952-4cdd-8e42-9af493cda467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Training Epoch 1/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [02:31<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Validating Epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Epoch [1/3] | Train Loss: 0.5278 | Val Loss: 0.5690 | Val Acc: 70.50%\n",
      "ðŸ’¾ Model saved (best so far)\n",
      "\n",
      "ðŸ§  Training Epoch 2/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [02:31<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Validating Epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Epoch [2/3] | Train Loss: 0.5239 | Val Loss: 0.4260 | Val Acc: 80.70%\n",
      "ðŸ’¾ Model saved (best so far)\n",
      "\n",
      "ðŸ§  Training Epoch 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [02:32<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Validating Epoch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Epoch [3/3] | Train Loss: 0.5402 | Val Loss: 0.4853 | Val Acc: 76.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Limit validation subset for faster run\n",
    "val_subset_size = min(1000, len(val_data))\n",
    "val_indices = np.random.choice(len(val_data), val_subset_size, replace=False)\n",
    "val_subset = torch.utils.data.Subset(val_data, val_indices)\n",
    "val_loader = DataLoader(val_subset, batch_size=BATCH, shuffle=False)\n",
    "\n",
    "EPOCHS = 3\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # ---------------------- TRAINING --------------------------\n",
    "    print(f\"\\nðŸ§  Training Epoch {epoch+1}/{EPOCHS}...\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Train {epoch+1}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # ---------------------- VALIDATION ------------------------\n",
    "    print(f\"ðŸ” Validating Epoch {epoch+1}...\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    # ---------------------- PRINT EPOCH RESULT ------------------------\n",
    "    print(f\"\\nâœ… Epoch [{epoch+1}/{EPOCHS}] | \"\n",
    "          f\"Train Loss: {running_loss/len(train_loader):.4f} | \"\n",
    "          f\"Val Loss: {val_loss/len(val_loader):.4f} | \"\n",
    "          f\"Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # ---------------------- SAVE BEST MODEL ---------------------------\n",
    "    if val_acc > best_val_acc:\n",
    "        torch.save(model.state_dict(), \"mini_deepfake_model.pth\")\n",
    "        best_val_acc = val_acc\n",
    "        print(\"ðŸ’¾ Model saved (best so far)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35661e1b-b63a-4ec7-a720-6739317b60cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ FAST TEST Accuracy on 1000 images: 67.40%\n"
     ]
    }
   ],
   "source": [
    "# FAST TEST EVALUATION (only 1000 images)\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Take 1000 random test images\n",
    "test_subset_size = min(1000, len(test_data))\n",
    "test_indices = np.random.choice(len(test_data), test_subset_size, replace=False)\n",
    "test_subset = Subset(test_data, test_indices)\n",
    "test_loader_fast = DataLoader(test_subset, batch_size=BATCH, shuffle=False)\n",
    "\n",
    "model.load_state_dict(torch.load(\"mini_deepfake_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader_fast:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"ðŸŽ¯ FAST TEST Accuracy on 1000 images: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b6a2b85-eeb9-4047-90df-ce635661adef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fake', 'Real']\n"
     ]
    }
   ],
   "source": [
    "print(train_data_full.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f279caad-f5d5-4e8b-ba78-d676190aa2d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpredict_image\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mpath/to/test/img.jpg\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'predict_image' is not defined"
     ]
    }
   ],
   "source": [
    " predict_image(\"path/to/test/img.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100b3782-b140-4c22-b9cd-48beedfeda6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
